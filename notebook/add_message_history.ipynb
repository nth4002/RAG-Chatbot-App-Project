{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f19318",
   "metadata": {},
   "source": [
    "[refrence link on how to build rag with chat history](https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/#chains)\n",
    "\n",
    "[MongoDB ChatHisotryMessage lastest api](https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/chat_message_histories/langchain_mongodb.chat_message_histories.MongoDBChatMessageHistory.html#langchain_mongodb.chat_message_histories.MongoDBChatMessageHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bda8e8",
   "metadata": {},
   "source": [
    "import all libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd4062d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from uuid import uuid4\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfa01e",
   "metadata": {},
   "source": [
    "create mongodb vector store and search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(), override=True)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "# gemini_embeddings\n",
    "\n",
    "MONGODB_ATLAS_CLUSTER_URI = os.getenv(\"MONGODB_ATLAS_CLUSTER_URI\")\n",
    "client = MongoClient(\n",
    "    MONGODB_ATLAS_CLUSTER_URI\n",
    ")\n",
    "DB_NAME = \"RAG-Chatbot-Cluster\"\n",
    "COLLECTION_NAME = \"RAG-Chatbot-Collection-Test\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"RAG-Chatbot-Index-Test\"\n",
    "\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=MONGODB_COLLECTION,\n",
    "    embedding=gemini_embeddings,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\"\n",
    ")\n",
    "\n",
    "def initialize_vector_store():\n",
    "    \"\"\"Initialize the MongoDB collection and verify the vector search index.\"\"\"\n",
    "    try:\n",
    "        # Verify MongoDB connection\n",
    "        client.server_info()  # Raises an exception if connection fails\n",
    "        logging.info(\"MongoDB connection established successfully\")\n",
    "\n",
    "        # Check if collection exists\n",
    "        if COLLECTION_NAME not in client[DB_NAME].list_collection_names():\n",
    "            client[DB_NAME].create_collection(COLLECTION_NAME)\n",
    "            logging.info(f\"Created collection {COLLECTION_NAME}\")\n",
    "        else:\n",
    "            logging.info(f\"Collection {COLLECTION_NAME} already exists\")\n",
    "\n",
    "\n",
    "        # Note: Vector search index must be created in MongoDB Atlas UI or via API\n",
    "        logging.info(f\"Ensure vector search index '{ATLAS_VECTOR_SEARCH_INDEX_NAME}' is configured in MongoDB Atlas for collection {COLLECTION_NAME}\")\n",
    "        create_index()\n",
    "        # vector_store.create_vector_search_index(\n",
    "        #     dimensions=768,\n",
    "        #     filters=[{\"type\":\"filter\", \"path\": \"source\"}],\n",
    "        #     update=True\n",
    "        # )\n",
    "        # Test vector store by adding a dummy document\n",
    "        dummy_doc = Document(page_content=\"Test document\", metadata={\"file_id\": 0})\n",
    "        vector_store.add_documents([dummy_doc])\n",
    "        logging.info(\"Added test document to vector store\")\n",
    "\n",
    "        # Log the inserted document to inspect its structure\n",
    "        inserted_doc = vector_store._collection.find_one({\"file_id\": 0})\n",
    "        if inserted_doc:\n",
    "            logging.info(f\"Inserted test document: {inserted_doc.get('file_id')}\")\n",
    "        else:\n",
    "            logging.error(\"Test document not found after insertion\")\n",
    "\n",
    "        # Delete the test document\n",
    "        result = vector_store._collection.delete_one({\"file_id\": 0})\n",
    "        if result.deleted_count > 0:\n",
    "            logging.info(\"Successfully deleted test document\")\n",
    "        else:\n",
    "            logging.warning(\"No test document was deleted; check document structure or query\")\n",
    "\n",
    "        # Verify deletion\n",
    "        remaining_doc = vector_store._collection.find_one({\".file_id\": 0})\n",
    "        if remaining_doc:\n",
    "            logging.error(f\"Test document still exists after deletion attempt: {remaining_doc}\")\n",
    "        else:\n",
    "            logging.info(\"Confirmed test document was deleted\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize vector store: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_index():\n",
    "    search_index_model = SearchIndexModel(\n",
    "                definition={\n",
    "                    \"mappings\": {\n",
    "                        \"dynamic\": True,\n",
    "                        \"fields\": {\n",
    "                            \"embedding\": {  # Correct structure: field name as key\n",
    "                                \"type\": \"knnVector\",\n",
    "                                \"dimensions\": 768,\n",
    "                                \"similarity\": \"cosine\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "            )\n",
    "    \n",
    "    result = MONGODB_COLLECTION.create_search_index(model=search_index_model)\n",
    "    logging.info(f\"Succesfully creating Atlas Search Index: {result}\")\n",
    "\n",
    "\n",
    "def delete_collection():\n",
    "    \"\"\"Delete the entire MongoDB collection.\"\"\"\n",
    "    try:\n",
    "        client[DB_NAME].drop_collection(COLLECTION_NAME)\n",
    "        logging.info(f\"Successfully deleted collection {COLLECTION_NAME}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error deleting collection {COLLECTION_NAME}: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "initialize_vector_store()\n",
    "# create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c48411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8229d8",
   "metadata": {},
   "source": [
    "load and split and add **document** to vector store "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c572df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['6808eb8e2f0953a2ef63db9d',\n",
       " '6808eb8e2f0953a2ef63db9e',\n",
       " '6808eb8e2f0953a2ef63db9f',\n",
       " '6808eb8e2f0953a2ef63dba0',\n",
       " '6808eb8e2f0953a2ef63dba1',\n",
       " '6808eb8e2f0953a2ef63dba2',\n",
       " '6808eb8e2f0953a2ef63dba3',\n",
       " '6808eb8e2f0953a2ef63dba4',\n",
       " '6808eb8e2f0953a2ef63dba5',\n",
       " '6808eb8e2f0953a2ef63dba6',\n",
       " '6808eb8e2f0953a2ef63dba7',\n",
       " '6808eb8e2f0953a2ef63dba8',\n",
       " '6808eb8e2f0953a2ef63dba9',\n",
       " '6808eb8e2f0953a2ef63dbaa',\n",
       " '6808eb8e2f0953a2ef63dbab',\n",
       " '6808eb8e2f0953a2ef63dbac',\n",
       " '6808eb8e2f0953a2ef63dbad',\n",
       " '6808eb8e2f0953a2ef63dbae',\n",
       " '6808eb8e2f0953a2ef63dbaf',\n",
       " '6808eb8e2f0953a2ef63dbb0',\n",
       " '6808eb8e2f0953a2ef63dbb1',\n",
       " '6808eb8e2f0953a2ef63dbb2',\n",
       " '6808eb8e2f0953a2ef63dbb3',\n",
       " '6808eb8e2f0953a2ef63dbb4',\n",
       " '6808eb8e2f0953a2ef63dbb5',\n",
       " '6808eb8e2f0953a2ef63dbb6',\n",
       " '6808eb8e2f0953a2ef63dbb7',\n",
       " '6808eb8e2f0953a2ef63dbb8',\n",
       " '6808eb8e2f0953a2ef63dbb9',\n",
       " '6808eb8e2f0953a2ef63dbba',\n",
       " '6808eb8e2f0953a2ef63dbbb',\n",
       " '6808eb8e2f0953a2ef63dbbc',\n",
       " '6808eb8e2f0953a2ef63dbbd',\n",
       " '6808eb8e2f0953a2ef63dbbe',\n",
       " '6808eb8e2f0953a2ef63dbbf',\n",
       " '6808eb8e2f0953a2ef63dbc0',\n",
       " '6808eb8e2f0953a2ef63dbc1',\n",
       " '6808eb8e2f0953a2ef63dbc2',\n",
       " '6808eb8e2f0953a2ef63dbc3',\n",
       " '6808eb8e2f0953a2ef63dbc4',\n",
       " '6808eb8e2f0953a2ef63dbc5',\n",
       " '6808eb8e2f0953a2ef63dbc6',\n",
       " '6808eb8e2f0953a2ef63dbc7',\n",
       " '6808eb8e2f0953a2ef63dbc8',\n",
       " '6808eb8e2f0953a2ef63dbc9',\n",
       " '6808eb8e2f0953a2ef63dbca',\n",
       " '6808eb8e2f0953a2ef63dbcb',\n",
       " '6808eb8e2f0953a2ef63dbcc',\n",
       " '6808eb8e2f0953a2ef63dbcd',\n",
       " '6808eb8e2f0953a2ef63dbce',\n",
       " '6808eb8e2f0953a2ef63dbcf',\n",
       " '6808eb8e2f0953a2ef63dbd0',\n",
       " '6808eb8e2f0953a2ef63dbd1',\n",
       " '6808eb8e2f0953a2ef63dbd2',\n",
       " '6808eb8e2f0953a2ef63dbd3',\n",
       " '6808eb8e2f0953a2ef63dbd4',\n",
       " '6808eb8e2f0953a2ef63dbd5',\n",
       " '6808eb8e2f0953a2ef63dbd6',\n",
       " '6808eb8e2f0953a2ef63dbd7',\n",
       " '6808eb8e2f0953a2ef63dbd8',\n",
       " '6808eb8e2f0953a2ef63dbd9',\n",
       " '6808eb8e2f0953a2ef63dbda',\n",
       " '6808eb8e2f0953a2ef63dbdb',\n",
       " '6808eb8e2f0953a2ef63dbdc',\n",
       " '6808eb8e2f0953a2ef63dbdd',\n",
       " '6808eb8e2f0953a2ef63dbde']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(len(splits))\n",
    "for split in splits:\n",
    "            # add file id to the metadata of each split\n",
    "    split.metadata['file_id'] = 0\n",
    "        \n",
    "        # add the document chunks to the vector store\n",
    "vector_store.add_documents(splits)\n",
    "# vector_store.add_documents(documents=splits, ids=[str(uuid4) for _ in range(len(splits))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84eae700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGODB_COLLECTION.distinct(key=\"sessionId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f75ef",
   "metadata": {},
   "source": [
    "Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdc2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-001\",\n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    ")\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "880fec6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Task Decomposition?',\n",
       " 'context': [],\n",
       " 'answer': 'Task decomposition is the process of breaking down a large, complex task into smaller, more manageable subtasks. This helps to simplify the task, making it easier to understand, plan, and execute. It also allows for more efficient allocation of resources and responsibilities. '}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c278a",
   "metadata": {},
   "source": [
    "# Addingn history message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cdfd421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520af1c",
   "metadata": {},
   "source": [
    "Adding question with chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7166573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be done in three common ways: \n",
      "\n",
      "1. **LLM with simple prompting:** The LLM is instructed to list steps or subgoals for completing the task. \n",
      "2. **Task-specific instructions:** The LLM is given instructions tailored to the specific task, such as \"Write a story outline\" for writing a novel.\n",
      "3. **Human inputs:** A human can provide a breakdown of the task, guiding the LLM on how to approach it. \n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd311f29",
   "metadata": {},
   "source": [
    "Here we've gone over how to add application logic for incorporating historical outputs, but we're still manually updating the chat history and inserting it into each input. In a real Q&A application we'll want some way of persisting chat history and some way of automatically inserting and updating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7b0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cced5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, simpler steps. This can be achieved through prompting techniques like Chain of Thought (CoT), which instructs the model to \"think step by step,\" or by using task-specific instructions like \"Write a story outline.\" '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d959a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be done in a few ways:\\n\\n1. **LLM with simple prompting:**  The LLM can be prompted with simple questions like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\". \\n2. **Task-specific instructions:** You can provide specific instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\\n3. **Human inputs:** Humans can directly provide the breakdown of tasks, especially when the task is highly complex or requires domain expertise. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44fc6701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is the process of breaking down a complex task into smaller, simpler steps. This can be achieved through prompting techniques like Chain of Thought (CoT), which instructs the model to \"think step by step,\" or by using task-specific instructions like \"Write a story outline.\" ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What are common ways of doing it?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition can be done in a few ways:\\n\\n1. **LLM with simple prompting:**  The LLM can be prompted with simple questions like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\". \\n2. **Task-specific instructions:** You can provide specific instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\\n3. **Human inputs:** Humans can directly provide the breakdown of tasks, especially when the task is highly complex or requires domain expertise. ', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2915d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is the process of breaking down a complex task into smaller, simpler steps. This can be achieved through prompting techniques like Chain of Thought (CoT), which instructs the model to \"think step by step,\" or by using task-specific instructions like \"Write a story outline.\" \n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Task decomposition can be done in a few ways:\n",
      "\n",
      "1. **LLM with simple prompting:**  The LLM can be prompted with simple questions like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\". \n",
      "2. **Task-specific instructions:** You can provide specific instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
      "3. **Human inputs:** Humans can directly provide the breakdown of tasks, especially when the task is highly complex or requires domain expertise. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
