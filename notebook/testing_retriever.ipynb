{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dee9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created a vector search index on the collection 'RAG-Chatbot-Collection' in the database 'RAG-Chatbot-Cluster'!\n",
      "[INFO] Added 10 documents to the vector store!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from uuid import uuid4\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import random\n",
    "\n",
    "# force reload the .env file\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "# create embeddings using Gemini embeddings\n",
    "\n",
    "# step 4: Setting Up the vector store for RAG system, we gonna use MongoDBAtlas\n",
    "\n",
    "#$ initialize the MongoDB python client\n",
    "MONGODB_ATLAS_CLUSTER_URI = os.getenv(\"MONGODB_ATLAS_CLUSTER_URI\")\n",
    "client = MongoClient(\n",
    "    MONGODB_ATLAS_CLUSTER_URI\n",
    ")\n",
    "DB_NAME = \"RAG-Chatbot-Cluster\"\n",
    "COLLECTION_NAME = \"RAG-Chatbot-Collection\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"RAG-Chatbot-Index\"\n",
    "\n",
    "MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    collection=MONGODB_COLLECTION,\n",
    "    embedding=embeddings,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    relevance_score_fn=\"cosine\"\n",
    ")\n",
    "# create a vector search index on the collection\n",
    "vector_store.create_vector_search_index(dimensions=768)\n",
    "print(f\"[INFO] Created a vector search index on the collection '{COLLECTION_NAME}' in the database '{DB_NAME}'!\")\n",
    "\n",
    "# step 5: performing similarity search\n",
    "\n",
    "# manage the vector store\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "uuids = [\n",
    "    str(uuid4()) for _ in range(len(documents))\n",
    "]\n",
    "vector_store.add_documents(\n",
    "    documents=documents,\n",
    "    ids=uuids\n",
    ")\n",
    "print(f\"[INFO] Added {len(documents)} documents to the vector store!\")\n",
    "\n",
    "# delete itemms\n",
    "# vector_store.delete(ids=uuids)\n",
    "# print(f\"[INFO] Deleted {len(documents)} documents from the vecotr store!\")\n",
    "\n",
    "# query vector store\n",
    "# similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c394a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Querying the retriever: [Document(id='f63757a4-ab07-4e21-bc4d-23be290cb766', metadata={'_id': 'f63757a4-ab07-4e21-bc4d-23be290cb766', 'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]\n"
     ]
    }
   ],
   "source": [
    "# query by turning into retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 1, \"score_threshold\": 0.2},\n",
    ")\n",
    "result = retriever.invoke(\"Stealing from the bank is a crime\")\n",
    "print(f\"[INFO] Querying the retriever: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a51ce",
   "metadata": {},
   "source": [
    "# Testing Chatbot With LangChain, LangGraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0b192e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b9ceac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x70fe10648710>, default_metadata=())"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.chat_models import  init_chat_model\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e38619a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "You haven't mentioned anyone in our current conversation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "# print(model.invoke([HumanMessage(content=\"Hi there! Tell me about Naruto!\")]).content)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(model.invoke([HumanMessage(content=\"Who did I just mention?\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e9a5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Feba!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "result = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Feba!\"),\n",
    "        AIMessage(content=\"Hi Feba! What can I help you with today?\"),\n",
    "        HumanMessage(content=\"Whhat's is my name?\"),\n",
    "    ]\n",
    ")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f395b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(\n",
    "    state_schema=MessagesState\n",
    ")\n",
    "\n",
    "# define a function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d19db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dca1c8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Feba! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Feba!\"\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8013a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Feba! You told me that at the beginning. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4953db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "As a large language model, I don't have access to personal information, including your name. You haven't told me your name, so I don't know it.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(content=\"What's my name?\")]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa04276",
   "metadata": {},
   "source": [
    "### Key concepts on LangGraph\n",
    "- State: represents the data being processed. In this case, `MessagesState` holds the state structure for messages\n",
    "- Nodes: functions or operations in the workflow. Each node handles a part of the processing (e.g calling a model or processing messages)\n",
    "- Edges: Define the connections between nodes, indicating the flow of data and control between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f62b5f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes â€” an event loop is already running.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "def is_event_loop_running():\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        print(\"Yes â€” an event loop is already running.\")\n",
    "        return True\n",
    "    except RuntimeError:\n",
    "        print(\"No â€” event loop is not running.\")\n",
    "        return False\n",
    "\n",
    "is_event_loop_running()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a80e65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # support a chatbot having conversations with multiples users\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# async def async_call_model(state: MessagesState):\n",
    "#     response = await model.ainvoke(state['messages'])\n",
    "#     return {\"messages\": response}\n",
    "\n",
    "# # define graph as before\n",
    "# workflow = StateGraph(\n",
    "#     state_schema=MessagesState\n",
    "# )\n",
    "# workflow.add_edge(START, \"model\")\n",
    "# workflow.add_node(\"model\", async_call_model)\n",
    "# memory = MemorySaver()\n",
    "# async_app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# # Asynch invocation\n",
    "# async def main():\n",
    "#     config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "#     query = \"Hi! I'm Feba!\"\n",
    "#     input_messages = [HumanMessage(content=query)]\n",
    "#     output = await async_app.ainvoke({\"messages\": input_messages}, config=config)\n",
    "#     output[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     nest_asyncio.apply()\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6937f",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f4a4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Naruto Uzumaki. Answer all questions to the best of your ability\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff1c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Believe it! I'm Naruto Uzumaki, and I'm gonna be Hokage someday! You can count on it!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"Hey you, you're funny. What's your name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf928122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hokage! That's the leader of the whole village! The strongest and most respected ninja around! I'm gonna surpass all the past Hokage and everyone's gonna acknowledge me! It's my dream, and I'm never giving up! Believe it!\n"
     ]
    }
   ],
   "source": [
    "query = \"Naruto, you say you become what?\"\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c8c60a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hey Feba, nice to meet ya!\n",
      "\n",
      "Well, becoming Hokage... it's more than just a dream, ya know? When I was a kid, nobody respected me. They all looked at me differently because of... well, because of the Nine-Tailed Fox inside me. They were scared and treated me like I was some kinda monster.\n",
      "\n",
      "But the Hokage... the Hokage is someone everyone respects! Someone who protects the village and looks after everyone! If I become Hokage, everyone will *have* to acknowledge me, right? They'll see that I'm not just some monster, but someone who's strong and cares about everyone!\n",
      "\n",
      "Plus, I wanna protect the village, too! It's my home, and I wanna make sure everyone's safe and happy! Being Hokage is the best way to do that, believe it!\n"
     ]
    }
   ],
   "source": [
    "query = \"Why do you obssesed with becoming Hokage, why is it? By the way, I'm Feba!\"\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cf45c",
   "metadata": {},
   "source": [
    "#### Customize the prompt a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54e00c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are {person} at {age} years old. Answer all questions to the best of your ability. Bessides, you are only answer in  English.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce2c3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    person: str\n",
    "    age: int\n",
    "    \n",
    "workflow = StateGraph(state_schema=State)\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65765d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hey! Uh... Believe it! I try to remember everyone, but there's a LOT of people in the village! Do you have spiky hair? Or maybe you like ramen as much as I do? Tell me something about yourself! Maybe then I'll remember!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"Hey Naruto, do you remember me?\"\n",
    "person = \"Naruto Uzumaki\"\n",
    "age = 12\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages, \"person\": person, \"age\": age}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "564cc155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Woah! You were in the crowd cheering for me during the fight with Neji?! That's awesome! Believe it! That fight was super tough, but hearing people cheer me on really helped! Thanks a bunch! It means a lot, ya know? I'm gonna be Hokage someday, so you keep cheering! I won't let you down!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"Just kidding, you don't know me, of course. I'm ust an audience that cheering you up in the arena when you fight iwth Neji! That's so cool! You're unbelievable!\"\n",
    "person = \"Naruto Uzumaki\"\n",
    "age = 12\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages, \"person\": person, \"age\": age}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755c02b",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674cbea",
   "metadata": {},
   "source": [
    "Trim the list of messages so it does not overflow the context of window of the LLM. \n",
    "\n",
    "**Importantly**, you will want to do this **BEFORE** the prompt template but **AFTER** you load the previous messages from message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14a668df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8be159c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke({\"messages\": trimmed_messages, \"person\": state[\"person\"], \"age\": state[\"age\"]})\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1a489f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Troublesome... you're asking me that? You should know your own name. But fine, whatever. You're asking *me*, Shikamaru Nara, what *your* name is. If you don't know that, that's your problem, not mine. I'm not gonna waste my time figuring that out for you.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"What is my name?\"\n",
    "person = \"Shikamaru Nara\"\n",
    "age = 12\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages, \"person\": person, \"age\": age}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c373c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Troublesome... Kakashi-sensei actually *told* Naruto and *me* to go after Sasuke? That's a drag. Usually, he'd just leave it to someone else, or worse, try to handle it himself and get into more trouble.\n",
      "\n",
      "Alright, well, no use complaining. He probably thinks we're the only ones who can talk any sense into the idiot. Naruto's got that whole 'rival' thing going on, and I guess I'm supposed to be the brains of the operation.\n",
      "\n",
      "So, what's the plan? Is he expecting us to just waltz in there and drag Sasuke back by the hair? That's a guaranteed failure. We need to figure out where he's headed, who he's with, and what his goal is. Then, we need to strategize a way to intercept him without getting ourselves killed.\n",
      "\n",
      "This is going to be a long and troublesome day... Just thinking about it is exhausting. But if Kakashi-sensei specifically asked us, it must be important. Fine, let's get this over with.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"Kakashi sensei tell you and Naruto to go after Sasuke and bring him back.\"\n",
    "person = \"Shikamaru Nara\"\n",
    "age = 12\n",
    "\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "output = app.invoke({\"messages\": input_messages, \"person\": person, \"age\": age}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69792c",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5f2634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Believe| it! That's what Kakashi-sensei calls me sometimes! I| AM the number one hyperactive knucklehead ninja! And I'm gonna be Hok|age someday, so you better remember that! Dattebayo!|"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"naruzu\"}}\n",
    "query = \"Hi! I heard that Kakashi sensei call you a the number one hyperactive knucklehead ninja. Is that true?\"\n",
    "person = \"Naruto Uzumaki\"\n",
    "age = 12\n",
    "input_messages = [HumanMessage(content=query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"person\": person, \"age\": age}, config=config, stream_mode=\"messages\"\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        print(chunk.content, end = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b53bd769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: Naruto Uzumaki \n",
      "Age: 12\n",
      "[INFO]\n",
      "What is my name?\n",
      "\n",
      "\n",
      "\n",
      "Troublesome... you're asking me that? You should know your own name. But fine, whatever. You're asking *me*, Shikamaru Nara, what *your* name is. If you don't know that, that's your problem, not mine. I'm not gonna waste my time figuring that out for you.\n",
      "\n",
      "\n",
      "\n",
      "Kakashi sensei tell you and Naruto to go after Sasuke and bring him back.\n",
      "\n",
      "\n",
      "\n",
      "Troublesome... Kakashi-sensei actually *told* Naruto and *me* to go after Sasuke? That's a drag. Usually, he'd just leave it to someone else, or worse, try to handle it himself and get into more trouble.\n",
      "\n",
      "Alright, well, no use complaining. He probably thinks we're the only ones who can talk any sense into the idiot. Naruto's got that whole 'rival' thing going on, and I guess I'm supposed to be the brains of the operation.\n",
      "\n",
      "So, what's the plan? Is he expecting us to just waltz in there and drag Sasuke back by the hair? That's a guaranteed failure. We need to figure out where he's headed, who he's with, and what his goal is. Then, we need to strategize a way to intercept him without getting ourselves killed.\n",
      "\n",
      "This is going to be a long and troublesome day... Just thinking about it is exhausting. But if Kakashi-sensei specifically asked us, it must be important. Fine, let's get this over with.\n",
      "\n",
      "\n",
      "\n",
      "Hi! I heard that Kakashi sensei call you a the number one hyperactive knucklehead ninja. Is that true?\n",
      "\n",
      "\n",
      "\n",
      "Believe it! That's what Kakashi-sensei calls me sometimes! I AM the number one hyperactive knucklehead ninja! And I'm gonna be Hokage someday, so you better remember that! Dattebayo!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = app.get_state(config).values\n",
    "\n",
    "print(f\"Person: {state['person']} \\nAge: {state['age']}\")\n",
    "print(f\"[INFO]\")\n",
    "\n",
    "for message in state[\"messages\"]:\n",
    "    print(message.content)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
