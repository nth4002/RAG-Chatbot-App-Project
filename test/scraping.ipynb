{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8b89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from typing import List, Dict\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6315ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, base_url: str, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize the WebScraper with configuration parameters.\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): The main URL to scrape\n",
    "            chunk_size (int): Size of text chunks for splitting\n",
    "            chunk_overlap (int): Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        self.visited_urls = set()\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging for the scraper\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if URL is valid and belongs to the same domain.\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to check\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if URL is valid, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            return parsed.netloc == self.domain\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean scraped text by removing extra whitespace and special characters.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to clean\n",
    "            \n",
    "        Returns:\n",
    "            str: Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def extract_text_from_html(self, html_content: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract meaningful text from HTML content.\n",
    "        \n",
    "        Args:\n",
    "            html_content (str): Raw HTML content\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for element in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "            element.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        return self.clean_text(text)\n",
    "\n",
    "    def get_links_from_html(self, html_content: str, current_url: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract all valid links from HTML content.\n",
    "        \n",
    "        Args:\n",
    "            html_content (str): Raw HTML content\n",
    "            current_url (str): Current page URL\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of valid URLs\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        links = []\n",
    "        \n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            absolute_url = urljoin(current_url, href)\n",
    "            \n",
    "            if self.is_valid_url(absolute_url) and absolute_url not in self.visited_urls:\n",
    "                links.append(absolute_url)\n",
    "        \n",
    "        return links\n",
    "\n",
    "    def scrape_url(self, url: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Scrape content from a single URL.\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL to scrape\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, str]: Dictionary containing URL and its content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Scraping URL: {url}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            text_content = self.extract_text_from_html(response.text)\n",
    "            return {\"url\": url, \"content\": text_content}\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_website(self, max_pages: int = 10) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Scrape the website starting from base_url.\n",
    "        \n",
    "        Args:\n",
    "            max_pages (int): Maximum number of pages to scrape\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict[str, str]]: List of dictionaries containing URLs and their content\n",
    "        \"\"\"\n",
    "        pages_to_visit = [self.base_url]\n",
    "        scraped_content = []\n",
    "        \n",
    "        while pages_to_visit and len(self.visited_urls) < max_pages:\n",
    "            current_url = pages_to_visit.pop(0)\n",
    "            \n",
    "            if current_url in self.visited_urls:\n",
    "                continue\n",
    "                \n",
    "            self.visited_urls.add(current_url)\n",
    "            content = self.scrape_url(current_url)\n",
    "            \n",
    "            if content:\n",
    "                scraped_content.append(content)\n",
    "                \n",
    "                # Get new links from the page\n",
    "                response = requests.get(current_url)\n",
    "                new_links = self.get_links_from_html(response.text, current_url)\n",
    "                pages_to_visit.extend(new_links)\n",
    "        \n",
    "        return scraped_content\n",
    "\n",
    "    def process_content(self, scraped_content: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Process scraped content and split into chunks.\n",
    "        \n",
    "        Args:\n",
    "            scraped_content (List[Dict[str, str]]): List of scraped content\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict[str, str]]: List of processed chunks with metadata\n",
    "        \"\"\"\n",
    "        processed_chunks = []\n",
    "        \n",
    "        for item in scraped_content:\n",
    "            chunks = self.text_splitter.split_text(item['content'])\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunks.append({\n",
    "                    'chunk_id': f\"{item['url']}_chunk_{i}\",\n",
    "                    'url': item['url'],\n",
    "                    'content': chunk\n",
    "                })\n",
    "        \n",
    "        return processed_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94b239b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:32:54,418 - INFO - Scraping URL: https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Example usage\n",
    "    base_url = \"https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/\"  # Replace with your target website\n",
    "    scraper = WebScraper(base_url)\n",
    "    \n",
    "    # Scrape the website\n",
    "    scraped_content = scraper.scrape_website(max_pages=1)\n",
    "    # return (scraped_content) \n",
    "    # Process and split the content\n",
    "    processed_chunks = scraper.process_content(scraped_content)\n",
    "    return processed_chunks \n",
    "    # # Print results\n",
    "    # print(f\"Total pages scraped: {len(scraped_content)}\")\n",
    "    # print(f\"Total chunks created: {len(processed_chunks)}\")\n",
    "    \n",
    "    # # Example of accessing the first chunk\n",
    "    # if processed_chunks:\n",
    "    #     print(\"\\nExample chunk:\")\n",
    "    #     print(f\"Chunk ID: {processed_chunks[0]['chunk_id']}\")\n",
    "    #     print(f\"URL: {processed_chunks[0]['url']}\")\n",
    "    #     print(f\"Content preview: {processed_chunks[0]['content'][:200]}...\")\n",
    "\n",
    "tmp = main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c9d1e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_0',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'Xây Dựng Hệ Thống RAG với DeepSeek R1  OllamaApidogNền tảng phát triển API hợp tác tất cả trong mộtThiết kế APITài liệu APIGỡ lỗi APIGiả lập APIKiểm thử API tự độngĐăng ký miễn phíTải xuốngCho Mac hoặc LinuxHomeXây Dựng Hệ Thống RAG với DeepSeek R1  OllamaNội dungHệ thống RAG cục bộ này tốn bao nhiêu?Những gì bạn sẽ cần1. Ollama2. Các biến thể mô hình DeepSeek R1Xây dựng Pipeline RAG Hướng dẫn mãBước 1 Nhập thư việnBước 2 Tải lên  Xử lý PDFBước 3 Chia nhỏ tài liệu một cách chiến lượcBước 4 Tạo một cơ sở dữ liệu tri thức có thể tìm kiếmBước 5 Cấu hình DeepSeek R1Bước 6 Lắp ráp chuỗi RAGBước 7 Khởi động giao diện webTương lai của RAG với DeepSeekNếu bạn từng ước có thể hỏi trực tiếp các câu hỏi về một tệp PDF hoặc hướng dẫn kỹ thuật, hướng dẫn này dành cho bạn. Hôm nay, chúng ta sẽ xây dựng một hệ thống Generative Tăng cường Lấy lại RAG sử dụng DeepSeek R1, một nguồn mở về lý luận, và Ollama, một khung nhẹ để chạy các mô hình AI cục bộ.Chuẩn bị tăng tốc thử nghiệm API của bạn? Đừng quên'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_1',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'một hệ thống Generative Tăng cường Lấy lại RAG sử dụng DeepSeek R1, một nguồn mở về lý luận, và Ollama, một khung nhẹ để chạy các mô hình AI cục bộ.Chuẩn bị tăng tốc thử nghiệm API của bạn? Đừng quên kiểm tra Apidog! Apidog hoạt động như một nền tảng một cửa để tạo, quản lý và chạy các thử nghiệm và máy chủ giả lập, giúp bạn xác định các nút cổ chai và giữ cho các API của bạn đáng tin cậy.Thay vì phải xử lý nhiều công cụ hoặc viết các kịch bản dài dòng, bạn có thể tự động hóa các phần quan trọng trong quy trình làm việc của mình, đạt được các pipeline CICD mượt mà và dành nhiều thời gian hơn để hoàn thiện các tính năng sản phẩm của mình.Nếu điều đó nghe có vẻ như điều gì đó có thể làm đơn giản hóa cuộc sống của bạn, hãy thử Apidog!buttonTrong bài viết này, chúng ta sẽ khám phá cách mà DeepSeek R1một mô hình cạnh tranh với o1 của OpenAI về hiệu suất nhưng có chi phí thấp hơn 95có thể làm tăng cường các hệ thống RAG của bạn. Hãy phân tích lý do tại sao các nhà phát triển đang đổ xô đến'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_2',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'R1một mô hình cạnh tranh với o1 của OpenAI về hiệu suất nhưng có chi phí thấp hơn 95có thể làm tăng cường các hệ thống RAG của bạn. Hãy phân tích lý do tại sao các nhà phát triển đang đổ xô đến công nghệ này và cách bạn có thể xây dựng pipeline RAG riêng của mình với nó.Hệ thống RAG cục bộ này tốn bao nhiêu? Thành phần Chi phí DeepSeek R1 1.5B Miễn phí Ollama Miễn phí PC RAM 16GB 0 Mô hình 1.5B của DeepSeek R1 nổi bật ở đây vìLấy lại có trọng tâm Chỉ có 3 khối tài liệu cung cấp cho mỗi câu trả lờiĐưa ra điều kiện nghiêm ngặt Tôi không biết ngăn chặn ảo giácThực hiện cục bộ Không có độ trễ so với các API trên đám mâyNhững gì bạn sẽ cầnTrước khi bắt đầu mã, chúng ta hãy thiết lập các bộ công cụ của mình1. OllamaOllama cho phép bạn chạy các mô hình như DeepSeek R1 cục bộ.Tải xuống httpsollama.comCài đặt, sau đó mở terminal của bạn và chạyollama run deepseek-r1  Đối với mô hình 7B mặc định button2. Các biến thể mô hình DeepSeek R1DeepSeek R1 có nhiều kích cỡ từ 1.5B đến 671B tham số. Đối'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_3',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'đặt, sau đó mở terminal của bạn và chạyollama run deepseek-r1  Đối với mô hình 7B mặc định button2. Các biến thể mô hình DeepSeek R1DeepSeek R1 có nhiều kích cỡ từ 1.5B đến 671B tham số. Đối với bản demo này, chúng ta sẽ sử dụng mô hình 1.5Bhoàn hảo cho RAG nhẹollama run deepseek-r11.5b Mẹo chuyên nghiệp Các mô hình lớn hơn như 70B cung cấp khả năng lý luận tốt hơn nhưng yêu cầu nhiều RAM hơn. Bắt đầu từ nhỏ, sau đó mở rộng!Xây dựng Pipeline RAG Hướng dẫn mãBước 1 Nhập thư việnChúng ta sẽ sử dụngLangChain Để xử lý và lấy lại tài liệuStreamlit Để giao diện webimport streamlit as st from langchain_community.document_loaders import PDFPlumberLoader from langchain_experimental.text_splitter import SemanticChunker from langchain_community.embeddings import HuggingFaceEmbeddings from langchain_community.vectorstores import FAISS from langchain_community.llms import Ollama Sơ đồ quy trình LangChain  StreamlitBước 2 Tải lên  Xử lý PDFTrong phần này, bạn sử dụng trình tải tệp của Streamlit để'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_4',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'import FAISS from langchain_community.llms import Ollama Sơ đồ quy trình LangChain  StreamlitBước 2 Tải lên  Xử lý PDFTrong phần này, bạn sử dụng trình tải tệp của Streamlit để cho phép người dùng chọn một tệp PDF cục bộ. Trình tải tệp Streamlit uploaded_file  st.file_uploaderTải lên một tệp PDF, typepdf if uploaded_file  Lưu PDF tạm thời with opentemp.pdf, wb as f f.writeuploaded_file.getvalue  Tải văn bản PDF loader  PDFPlumberLoadertemp.pdf docs  loader.load Khi đã tải lên, hàm PDFPlumberLoader sẽ trích xuất văn bản từ tệp PDF, chuẩn bị cho giai đoạn tiếp theo của pipeline. Phương pháp này rất thuận tiện vì nó xử lý việc đọc nội dung tệp mà không cần phải phân tích thủ công nhiều.Bước 3 Chia nhỏ tài liệu một cách chiến lượcChúng ta muốn sử dụng RecursiveCharacterTextSplitter, mã này chia nhỏ văn bản PDF gốc thành các đoạn nhỏ hơn khối. Hãy giải thích các khái niệm của việc chia nhỏ tốt vs xấu ở đâySo sánh bên cạnh nhau của việc chia nhỏ văn bản tốt vs. xấuTại sao lại là chia nhỏ'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_5',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'chia nhỏ văn bản PDF gốc thành các đoạn nhỏ hơn khối. Hãy giải thích các khái niệm của việc chia nhỏ tốt vs xấu ở đâySo sánh bên cạnh nhau của việc chia nhỏ văn bản tốt vs. xấuTại sao lại là chia nhỏ ngữ nghĩa?Nhóm các câu liên quan ví dụ Cách Milvus lưu trữ dữ liệu giữ nguyênTránh việc chia nhỏ các bảng hoặc hình vẽ Chia văn bản thành các khối ngữ nghĩa text_splitter  SemanticChunkerHuggingFaceEmbeddings documents  text_splitter.split_documentsdocs Bước này giữ lại bối cảnh bằng cách chồng chéo các đoạn một chút, giúp mô hình ngôn ngữ trả lời câu hỏi chính xác hơn. Những khối tài liệu nhỏ, được xác định tốt cũng làm cho việc tìm kiếm trở nên hiệu quả và liên quan hơn.Bước 4 Tạo một cơ sở dữ liệu tri thức có thể tìm kiếmSau khi chia nhỏ, pipeline sẽ tạo ra các vector embeddings cho các đoạn và lưu trữ chúng trong chỉ mục FAISS. Tạo embeddings embeddings  HuggingFaceEmbeddings vector_store  FAISS.from_documentsdocuments, embeddings  Kết nối retriever retriever'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_6',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'embeddings cho các đoạn và lưu trữ chúng trong chỉ mục FAISS. Tạo embeddings embeddings  HuggingFaceEmbeddings vector_store  FAISS.from_documentsdocuments, embeddings  Kết nối retriever retriever  vector_store.as_retrieversearch_kwargsk 3  Lấy 3 khối hàng đầu Điều này biến văn bản thành đại diện số mà dễ dàng truy vấn hơn nhiều. Các truy vấn sau đó sẽ chạy trên chỉ mục này để tìm các khối phù hợp nhất về ngữ cảnh.Bước 5 Cấu hình DeepSeek R1Tại đây, bạn khởi tạo một chuỗi RetrievalQA sử dụng Deepseek R1 1.5B như một mô hình ngôn ngữ cục bộ.llm  Ollamamodeldeepseek-r11.5b  Mô hình tham số 1.5B của chúng ta  Tạo mẫu câu hỏi prompt   1. Chỉ sử dụng bối cảnh bên dưới. 2. Nếu không chắc chắn, hãy nói Tôi không biết. 3. Giữ cho câu trả lời dưới 4 câu. Bối cảnh context Câu hỏi question Câu trả lời  QA_CHAIN_PROMPT  PromptTemplate.from_templateprompt Mẫu này buộc mô hình phải dựa vào nội dung trong PDF của bạn để trả lời. Bằng cách bao bọc mô hình ngôn ngữ với một retriever liên kết với chỉ'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_7',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'lời  QA_CHAIN_PROMPT  PromptTemplate.from_templateprompt Mẫu này buộc mô hình phải dựa vào nội dung trong PDF của bạn để trả lời. Bằng cách bao bọc mô hình ngôn ngữ với một retriever liên kết với chỉ mục FAISS, bất kỳ truy vấn nào được thực hiện qua chuỗi sẽ tìm kiếm bối cảnh từ nội dung PDF, khiến câu trả lời được dựa trên tài liệu gốc.Bước 6 Lắp ráp chuỗi RAGTiếp theo, bạn có thể kết nối các bước tải lên, chia nhỏ và lấy lại thành một pipeline hợp lý. Chuỗi 1 Tạo câu trả lời llm_chain  LLMChainllmllm, promptQA_CHAIN_PROMPT  Chuỗi 2 Kết hợp các khối tài liệu document_prompt  PromptTemplate templateBối cảnhncontentpage_contentnsourcesource, input_variablespage_content, source   Pipeline RAG cuối cùng qa  RetrievalQA combine_documents_chainStuffDocumentsChain llm_chainllm_chain, document_promptdocument_prompt , retrieverretriever  Đây là phần lõi của thiết kế RAG Generative Tăng cường Lấy lại, cung cấp cho mô hình ngôn ngữ lớn bối cảnh đã xác minh thay vì chỉ dựa vào việc đào tạo nội'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_8',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': ', retrieverretriever  Đây là phần lõi của thiết kế RAG Generative Tăng cường Lấy lại, cung cấp cho mô hình ngôn ngữ lớn bối cảnh đã xác minh thay vì chỉ dựa vào việc đào tạo nội bộ của nó.Bước 7 Khởi động giao diện webCuối cùng, mã sử dụng các hàm nhập và ghi của Streamlit để người dùng có thể nhập câu hỏi và xem phản hồi ngay lập tức. Giao diện Streamlit user_input  st.text_inputHãy hỏi PDF của bạn một câu hỏi if user_input with st.spinnerĐang suy nghĩ... response  qauser_inputresult st.writeresponse Ngay khi người dùng nhập truy vấn, chuỗi sẽ lấy lại các khối phù hợp nhất, đưa chúng vào mô hình ngôn ngữ, và hiển thị một câu trả lời. Với thư viện langchain được cài đặt đúng cách, mã này sẽ hoạt động mà không gây ra lỗi thiếu mô-đun.Hãy hỏi và gửi câu hỏi và nhận câu trả lời ngay lập tức!Dưới đây là mã hoàn chỉnhTương lai của RAG với DeepSeekVới các tính năng như tự xác minh và lý luận đa bước đang được phát triển, DeepSeek R1 sẵn sàng mở khóa thêm nhiều ứng dụng RAG tiên tiến hơn.'},\n",
       " {'chunk_id': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/_chunk_9',\n",
       "  'url': 'https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/',\n",
       "  'content': 'đây là mã hoàn chỉnhTương lai của RAG với DeepSeekVới các tính năng như tự xác minh và lý luận đa bước đang được phát triển, DeepSeek R1 sẵn sàng mở khóa thêm nhiều ứng dụng RAG tiên tiến hơn. Hãy tưởng tượng một AI không chỉ trả lời câu hỏi mà còn tranh luận về logic của chính nómột cách tự chủ.Giải pháp 1 cho Quản lý APIThiết kế APITài liệu APIGỡ lỗi APIKiểm thử API tự độngGiả lập APIThêmBắt đầu miễn phíHot TopicsPostman alternativePostman send jsonPostman collection runner limiAPI Upload fileWebsocket connection to failedFree API for testingSwagger alternativesSwagger UISwagger PHPSwagger localhost url'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8db862dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "https://apidog.com/vi/blog/rag-deepseek-r1-ollama-vi/\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "for chunk in tmp:\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content = chunk.get('content'),\n",
    "            metadata = {\"source\": chunk.get('url')}\n",
    "        )\n",
    "    )\n",
    "print(len(documents))\n",
    "print(documents[0].metadata['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
